"use strict";(self.webpackChunkleguru=self.webpackChunkleguru||[]).push([[331],{8127:function(e,t,a){a.r(t),a.d(t,{default:function(){return c}});var r=a(7294),o=a(1415),s=a.p+"static/m1-spiral-policy-ce6ff7e02bb56ab3393f8cd8633ca25b.mp4",i=a.p+"static/m1-spiral-policy-path-e6ebbf1316c0500a046627fa1092c1fc.mp4",n=a(7334);var c=function(){return r.createElement(n.Z,{className:"pt-10 pb-10",title:"Smol Strats"},r.createElement("div",null,r.createElement("h4",{className:"text-secondary text-3xl hover:opacity-50"},"Programmatic Reinforcement Learning (April 2023 - August 2023)"),r.createElement("p",{className:"text-accent-dark pt-5 text-lg"},"Worked with"," ",r.createElement(o.Z,{href:"https://games-automata-play.com/"},"NathanaÃ«l Fijalkow")," ","at the"," ",r.createElement(o.Z,{href:"https://www.mimuw.edu.pl/en/dziedziny-badan/teoria-automatow"},"automata theory group")," ","of the University of Warsaw on synthesizing programmatic policies for markov decision processes (MDPs) by exploiting programmatic representations of MDPs."),r.createElement("div",{className:"flex flex-row text-lg mt-1"},r.createElement(o.Z,{href:"https://github.com/guruprerana/guruprerana.github.io/raw/source/static/report-m1-internship.pdf",className:"ml-1"},"[Report]"))),r.createElement("video",{controls:!0,className:"mt-5"},r.createElement("source",{src:i,type:"video/mp4"})),r.createElement("p",{className:"text-accent-dark pt-5 text-lg"},r.createElement("b",null,"Abstract. "),"Starting from a programmatic representation of a markov decision process (MDP) in the"," ",r.createElement(o.Z,{href:"https://www.prismmodelchecker.org/manual/ThePRISMLanguage/Introduction"},"PRISM syntax"),", we examine the task of synthesizing a policy in the form of a program for the MDP. The PRISM syntax allows us to specify MDPs concisely by partitioning the state space into regions with similar actions and transitions. While we cannot address the complete expressive power of the PRISM syntax, we restrict ourselves to a small subclass of two dimensional deterministic gridworlds partitioned into regions along linear predicates. Using a relaxation of this class of gridworlds, we present an algorithm to synthesize programmatic policies which exploit the symmetries present in the representation of the MDP. Our programs use memory to track subgoals and navigate between the edges of regions to provide a concise representation of a policy. Our main result is a proof of a concrete upper bound on the size of the synthesized programs. We also give a practical implementation of our synthesis algorithm which is evaluated on randomly generated instances of gridworlds."),r.createElement("p",{className:"text-accent-dark pt-2 text-lg"},"A policy synthesized for a gridworld by our implementation can be visualized in the video below."),r.createElement("video",{controls:!0,className:"mt-5"},r.createElement("source",{src:s,type:"video/mp4"})))}}}]);
//# sourceMappingURL=component---src-pages-smol-strats-js-72033481ecb741c4be3a.js.map